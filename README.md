# Awesome Muon[![Awesome](https://awesome.re/badge.svg)](https://awesome.re)

This repo collects papers, documents, and codes about muon optimizer for anyone who wants to research it. We are continuously improving the project. Welcome to PR the works (papers, repositories) that the repo misses.

## Table of Contents

- [Papers](#papers)
  - [2025](#2025)
  - [2024](#2024)

### 2025
- [[NeurIPS](https://openreview.net/forum?id=Ei6IsmxYrb)] How to Scale Second-Order Optimization
- [[NeurIPS ER Workshop](https://openreview.net/forum?id=NHM0lL832y)] Muon: Training and Trade-offs with Latent Attention and MoE
- [[High-dimensional Learning Dynamics at ICML Poster](https://openreview.net/forum?id=ppmyFtr9EW)] Towards Understanding Orthogonalization in Muon [[code](https://anonymous.4open.science/r/MuonSBW-23A2)]
- [[ICLR 2026 Conference Submission](https://openreview.net/forum?id=g2l9bg9DWx)] Achieving low-bit Muon through subspace preservation and grid quantization
- [[ICLR 2026 Conference Submission](https://openreview.net/forum?id=lJSfxtLpLm)] Convergence of Muon with Newton-Schulz
- [[ICLR 2026 Conference Submission](https://openreview.net/forum?id=rex7s82Iav)] Error Feedback for Muon and Friends
- [[ICLR 2026 Conference Submission](https://openreview.net/forum?id=go388T3QjQ)] Long-tailed Learning with Muon Optimizer
- [[ICLR 2026 Conference Submission](https://openreview.net/forum?id=WtbXgc9GVA)] LoRA meets Riemannion: Muon Optimizer for Parametrization-independent Low-Rank Adapters
- [[ICLR 2026 Conference Submission](https://openreview.net/forum?id=mHouLSUQP5)] MuonBP: Faster Muon via Block-Periodic Orthogonalization
- [[ICLR 2026 Conference Submission](https://openreview.net/forum?id=7TeJXgr7L6)] NorMuon: Making Muon more efficient and scalable
- [[ICLR 2026 Conference Submission](https://openreview.net/forum?id=TpxkCwftHF)] On quantizing the state of the Muon optimizer
- [[ICLR 2026 Conference Submission](https://openreview.net/forum?id=CPhda7grEo)] On the Convergence of Muon and Beyond
- [[arXiv](https://arxiv.org/abs/2502.02900)] A Note on the Convergence of Muon
- [[arXiv](https://arxiv.org/abs/2507.11005)] AdaMuon: Adaptive Muon Optimizer
- [[arXiv](https://arxiv.org/abs/2509.02981)] AdaGrad Meets Muon: Adaptive Stepsizes for Orthogonal Updates
- [[arXiv](https://arxiv.org/abs/2510.09827)] An Exploration of Non-Euclidean Gradient Descent: Muon and its Many Variants
- [[arXiv](https://arxiv.org/abs/2510.19933)] Beyond the Ideal: Analyzing the Inexact Muon Update
- [[arXiv](https://arxiv.org/abs/2507.01598)] Convergence Bound and Critical Batch Size of Muon Optimizer
- [[arXiv](https://arxiv.org/abs/2502.17410)] COSMOS: A Hybrid Adaptive Optimizer for Memory-Efficient Training of LLMs
- [[arXiv](https://arxiv.org/abs/2510.01377)] DeMuon: A Decentralized Muon for Matrix Optimization over Graphs
- [[arXiv](https://arxiv.org/abs/2510.02239)] Drop-Muon: Update Less, Converge Faster
- [[arXiv](https://arxiv.org/abs/2510.00643)] Error Feedback for Muon and Friends
- [[arXiv](https://arxiv.org/abs/2509.23106)] Effective Quantization of Muon Optimizer States
- [[arXiv](https://arxiv.org/abs/2509.02046)] Fantastic Pretraining Optimizers and Where to Find Them
- [[arXiv](https://arxiv.org/abs/2505.13416)] Gluon: Making Muon & Scion Great Again! (Bridging Theory and Practice of LMO-based Optimizers for LLMs)
- [[arXiv](https://arxiv.org/abs/2510.22980)] How Muon's Spectral Design Benefits Generalization: A Study on Imbalanced Data
- [[arXiv](https://arxiv.org/abs/2502.04664)] Implicit Bias of Spectral Descent and Muon on Multiclass Separable Data
- [[arXiv](https://arxiv.org/abs/2509.14562)] LiMuon: Light and Fast Muon Optimizer for Large Models
- [[arXiv](https://arxiv.org/abs/2506.04192)] Lions and Muons: Optimization via Stochastic Frank-Wolfe
- [[arXiv](https://arxiv.org/abs/2509.11983)] Low-rank Orthogonalization for Large-scale Matrix Optimization with Applications to Foundation Model Training
- [[arXiv](https://arxiv.org/abs/2506.04430)] Leveraging Coordinate Momentum in SignSGD and Muon: Memory-Optimized Zero-Order
- [[arXiv](https://arxiv.org/abs/2502.16982)] Muon is Scalable for LLM Training [[code](https://github.com/MoonshotAI/Moonlight)]
- [[arXiv](https://arxiv.org/abs/2509.26030)] Muon Outperforms Adam in Tail-End Associative Memory Learning
- [[arXiv](https://arxiv.org/abs/2504.08451)] Muon-Accelerated Attention Distillation for Real-Time Edge Synthesis via Optimized Latent Diffusion
- [[arXiv](https://arxiv.org/abs/2504.16041)] Muon Optimizer Accelerates Grokking
- [[arXiv](https://arxiv.org/abs/2506.15054)] Muon Optimizes Under Spectral Norm Constraints
- [[arXiv](https://arxiv.org/abs/2505.23725)] MuLoCo: Muon is a practical inner optimizer for DiLoCo
- [[arXiv](https://arxiv.org/abs/2510.03866)] On Provable Benefits of Muon in Federated Learning
- [[arXiv](https://arxiv.org/abs/2505.02222)] Practical Efficiency of Muon for Pretraining
- [[arXiv](https://arxiv.org/abs/2510.06627)] POME: Post Optimization Model Edit via Muon-style Projection [[code](https://github.com/NUS-HPC-AI-Lab/POME)]
- [[arXiv](https://arxiv.org/abs/2510.03691)] REG: A Regularization Optimizer for Robust Training Dynamics
- [[arXiv](https://arxiv.org/abs/2505.16932)] The Polar Express: Optimal Matrix Sign Methods and Their Application to the Muon Algorithm
- [[arXiv](https://arxiv.org/abs/2503.12645)] Understanding Gradient Orthogonalization for Deep Learning via Non-Euclidean Trust-Region Optimization
- [[arXiv](https://arxiv.org/abs/2510.25000)] What Really Matters in Matrix-Whitening Optimizers?
  

### 2024
- [[URL](https://kellerjordan.github.io/posts/muon/)] Muon: An optimizer for hidden layers in neural networks

## Star History

[![Star History Chart](https://api.star-history.com/svg?repos=HaoKun-Li/Awesome-Muon&type=Timeline)](https://star-history.com/#HaoKun-Li/Awesome-Muon&Timeline)
