# Awesome Muon[![Awesome](https://awesome.re/badge.svg)](https://awesome.re)

This repo collects papers, documents, and codes about muon optimizer for anyone who wants to research it. We are continuously improving the project. Welcome to PR the works (papers, repositories) that the repo misses.

## Table of Contents

- [Papers](#papers)
  - [2025](#2025)
  - [2024](#2024)

### 2025
- [[NeurIPS](https://openreview.net/forum?id=Ei6IsmxYrb)] How to Scale Second-Order Optimization
- [[NeurIPS ER Workshop](https://openreview.net/forum?id=NHM0lL832y)] Muon: Training and Trade-offs with Latent Attention and MoE
- [[High-dimensional Learning Dynamics at ICML Poster](https://openreview.net/forum?id=ppmyFtr9EW)] Towards Understanding Orthogonalization in Muon [[code](https://anonymous.4open.science/r/MuonSBW-23A2)]
- [[ICLR 2026 Conference Submission](https://openreview.net/forum?id=g2l9bg9DWx)] Achieving low-bit Muon through subspace preservation and grid quantization
- [[ICLR 2026 Conference Submission](https://openreview.net/forum?id=lJSfxtLpLm)] Convergence of Muon with Newton-Schulz
- [[ICLR 2026 Conference Submission](https://openreview.net/forum?id=rex7s82Iav)] Error Feedback for Muon and Friends
- [[ICLR 2026 Conference Submission](https://openreview.net/forum?id=go388T3QjQ)] Long-tailed Learning with Muon Optimizer
- [[ICLR 2026 Conference Submission](https://openreview.net/forum?id=WtbXgc9GVA)] LoRA meets Riemannion: Muon Optimizer for Parametrization-independent Low-Rank Adapters
- [[ICLR 2026 Conference Submission](https://openreview.net/forum?id=mHouLSUQP5)] MuonBP: Faster Muon via Block-Periodic Orthogonalization
- [[ICLR 2026 Conference Submission](https://openreview.net/forum?id=7TeJXgr7L6)] NorMuon: Making Muon more efficient and scalable
- [[ICLR 2026 Conference Submission](https://openreview.net/forum?id=TpxkCwftHF)] On quantizing the state of the Muon optimizer
- [[ICLR 2026 Conference Submission](https://openreview.net/forum?id=CPhda7grEo)] On the Convergence of Muon and Beyond
- [[SSRN](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5514039)] AuON: A Survey For Linear-time Orthogonal Optimizer
- [[arXiv](https://arxiv.org/abs/2502.02900)] A Note on the Convergence of Muon
- [[arXiv](https://arxiv.org/abs/2506.16659)] A Minimalist Optimizer Design for LLM Pretraining
- [[arXiv](https://arxiv.org/abs/2507.11005)] AdaMuon: Adaptive Muon Optimizer
- [[arXiv](https://arxiv.org/abs/2509.02981)] AdaGrad Meets Muon: Adaptive Stepsizes for Orthogonal Updates
- [[arXiv](https://arxiv.org/abs/2510.09827)] An Exploration of Non-Euclidean Gradient Descent: Muon and its Many Variants
- [[arXiv](https://arxiv.org/abs/2503.20762)] ASGO: Adaptive Structured Gradient Optimization
- [[arXiv](https://arxiv.org/abs/2510.21314)] A Convergence Analysis of Adaptive Optimizers under Floating-point Quantization
- [[arXiv](https://arxiv.org/abs/2510.19933)] Beyond the Ideal: Analyzing the Inexact Muon Update
- [[arXiv](https://arxiv.org/abs/2509.01440)] Benchmarking Optimizers for Large Language Model Pretraining
- [[arXiv](https://arxiv.org/abs/2507.01598)] Convergence Bound and Critical Batch Size of Muon Optimizer
- [[arXiv](https://arxiv.org/abs/2502.17410)] COSMOS: A Hybrid Adaptive Optimizer for Memory-Efficient Training of LLMs
- [[arXiv](https://arxiv.org/abs/2509.24218)] Conda: Column-Normalized Adam for Training Large Language Models Faster
- [[arXiv](https://arxiv.org/abs/2510.01377)] DeMuon: A Decentralized Muon for Matrix Optimization over Graphs
- [[arXiv](https://arxiv.org/abs/2510.02239)] Drop-Muon: Update Less, Converge Faster
- [[arXiv](https://arxiv.org/abs/2509.18396)] Development of Deep Learning Optimizers: Approaches, Concepts, and Update Rules
- [[arXiv](https://arxiv.org/abs/2510.00643)] Error Feedback for Muon and Friends
- [[arXiv](https://arxiv.org/abs/2509.23106)] Effective Quantization of Muon Optimizer States
- [[arXiv](https://arxiv.org/abs/2509.15874)] ENSAM: an efficient foundation model for interactive segmentation of 3D medical images
- [[arXiv](https://arxiv.org/abs/2509.02046)] Fantastic Pretraining Optimizers and Where to Find Them
- [[arXiv](https://arxiv.org/abs/2511.04456)] Federated Stochastic Minimax Optimization under Heavy-Tailed Noises
- [[arXiv](https://arxiv.org/abs/2510.27403)] FedMuon: Accelerating Federated Learning with Matrix Orthogonalization
- [[arXiv](https://arxiv.org/abs/2509.26337)] FedMuon: Federated Learning with Bias-corrected LMO-based Optimization
- [[arXiv](https://arxiv.org/abs/2505.13416)] Gluon: Making Muon & Scion Great Again! (Bridging Theory and Practice of LMO-based Optimizers for LLMs)
- [[arXiv](https://arxiv.org/abs/2510.22980)] How Muon's Spectral Design Benefits Generalization: A Study on Imbalanced Data
- [[arXiv](https://arxiv.org/abs/2502.04664)] Implicit Bias of Spectral Descent and Muon on Multiclass Separable Data
- [[arXiv](https://arxiv.org/abs/2511.00674)] Isotropic Curvature Model for Understanding Deep Learning Optimization: Is Gradient Orthogonalization Optimal?
- [[arXiv](https://arxiv.org/abs/2507.20534)] Kimi K2: Open Agentic Intelligence
- [[arXiv](https://arxiv.org/abs/2509.14562)] LiMuon: Light and Fast Muon Optimizer for Large Models
- [[arXiv](https://arxiv.org/abs/2506.04192)] Lions and Muons: Optimization via Stochastic Frank-Wolfe
- [[arXiv](https://arxiv.org/abs/2509.11983)] Low-rank Orthogonalization for Large-scale Matrix Optimization with Applications to Foundation Model Training
- [[arXiv](https://arxiv.org/abs/2506.04430)] Leveraging Coordinate Momentum in SignSGD and Muon: Memory-Optimized Zero-Order
- [[arXiv](https://arxiv.org/abs/2502.16982)] Muon is Scalable for LLM Training [[code](https://github.com/MoonshotAI/Moonlight)]
- [[arXiv](https://arxiv.org/abs/2509.26030)] Muon Outperforms Adam in Tail-End Associative Memory Learning
- [[arXiv](https://arxiv.org/abs/2504.08451)] Muon-Accelerated Attention Distillation for Real-Time Edge Synthesis via Optimized Latent Diffusion
- [[arXiv](https://arxiv.org/abs/2504.16041)] Muon Optimizer Accelerates Grokking
- [[arXiv](https://arxiv.org/abs/2506.15054)] Muon Optimizes Under Spectral Norm Constraints
- [[arXiv](https://arxiv.org/abs/2505.23725)] MuLoCo: Muon is a practical inner optimizer for DiLoCo
- [[arXiv](https://arxiv.org/abs/2510.14009)] Noise-Adaptive Layerwise Learning Rates: Accelerating Geometry-Aware Optimization for Deep Neural Network Training
- [[arXiv](https://arxiv.org/abs/2510.03866)] On Provable Benefits of Muon in Federated Learning
- [[arXiv](https://arxiv.org/abs/2505.02222)] Practical Efficiency of Muon for Pretraining
- [[arXiv](https://arxiv.org/abs/2510.06627)] POME: Post Optimization Model Edit via Muon-style Projection [[code](https://github.com/NUS-HPC-AI-Lab/POME)]
- [[arXiv](https://arxiv.org/abs/2510.03691)] REG: A Regularization Optimizer for Robust Training Dynamics
- [[arXiv](https://arxiv.org/abs/2505.16932)] The Polar Express: Optimal Matrix Sign Methods and Their Application to the Muon Algorithm
- [[arXiv](https://arxiv.org/abs/2503.12645)] Understanding Gradient Orthogonalization for Deep Learning via Non-Euclidean Trust-Region Optimization
- [[arXiv](https://arxiv.org/abs/2510.25000)] What Really Matters in Matrix-Whitening Optimizers?
  

### 2024
- [[URL](https://kellerjordan.github.io/posts/muon/)] Muon: An optimizer for hidden layers in neural networks

## Star History

[![Star History Chart](https://api.star-history.com/svg?repos=HaoKun-Li/Awesome-Muon&type=Timeline)](https://star-history.com/#HaoKun-Li/Awesome-Muon&Timeline)
